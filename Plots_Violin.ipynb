{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP Sensitivity Analysis - Violin Plot Version\n",
    "\n",
    "Refactored analysis with publication-quality plots.\n",
    "\n",
    "**Structure:**\n",
    "1. Data Loading (Cells 1-5)\n",
    "2. Analysis Execution (Cell 6)\n",
    "3. Plots 1-a through 3-b (Cells 7-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Cell 1: Global Configuration\n",
    "# ========================================================\n",
    "\n",
    "# CHANGE DATASET HERE\n",
    "DATASET = 'german'  # Options: 'acs','diabetes', 'german'\n",
    "\n",
    "# NOTE: Models are configured in plot_violin.py Config.MODELS\n",
    "# Current models: ['dt', 'rf', 'xgb', 'ftt', 'mlp', 'tabpfn']\n",
    "# (logistic regression 'lr' has been removed, 'mlp' added between 'ftt' and 'tabpfn')\n",
    "\n",
    "# NOTE: For cross-dataset comparison, set these y-axis limits in plot_violin.py\n",
    "# after running all 3 datasets to find the maximum values:\n",
    "# \n",
    "# Config.L2_YLIM = (0, 2.5)  # Example: unified y-limit for L2 distance plots\n",
    "# Config.FEATURE_L2_YLIM = (0, 0.5)  # Example: unified x-limit for feature-wise plots\n",
    "# \n",
    "# This ensures consistent scales across acs, german, and diabetes datasets.\n",
    "\n",
    "print(f\"üìä Dataset: {DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Cell 2: Setup and Imports\n",
    "# ========================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import Javascript\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Disable scrolling\n",
    "display(Javascript('IPython.OutputArea.prototype._should_scroll = function(lines) {return false;}'))\n",
    "\n",
    "# Force reload if already imported\n",
    "if 'plot_violin' in sys.modules:\n",
    "    del sys.modules['plot_violin']\n",
    "\n",
    "# Import refactored module\n",
    "from plot_violin import *\n",
    "\n",
    "# Override Config.DATASET with notebook variable\n",
    "Config.DATASET = DATASET\n",
    "\n",
    "print(\"‚úÖ Imports successful!\")\n",
    "print(f\"Dataset: {Config.DATASET}\")\n",
    "print(f\"Models: {Config.MODELS}\")\n",
    "print(f\"Module location: {sys.modules['plot_violin'].__file__}\")\n",
    "print(\"\\nüìù Recent changes:\")\n",
    "print(\"  - Baselines: Using Monte Carlo (Mallows) for Jaccard/RBO, analytical for L2\")\n",
    "print(\"  - Visualization: Shaded region (min-max) across parameter sweeps\")\n",
    "print(\"  - Parameters: rho=[0.5,0.6,0.7], kappa=[5..15], q=[0.2,0.3,0.4]\")\n",
    "print(\"  - All baselines enabled (L2, Jaccard, RBO)\")\n",
    "print(\"  - RBO: p parameter now computed dynamically as p=1-(1/n_features)\")\n",
    "print(\"  - RBO: Uses full feature depth (r=d) for both baseline and distance calculations\")\n",
    "print(\"  - RBO: Properly computes distance as (1 - similarity) everywhere\")\n",
    "print(\"  - Feature sorting: descending order (largest at bottom for 1-d)\")\n",
    "print(\"  - Plot 2-b, 3-b: reversed lists (descending from top to bottom)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Cell 3: Load Global Data (X, Y)\n",
    "# ========================================================\n",
    "\n",
    "with open(f'./dataset/{Config.DATASET}_X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "\n",
    "with open(f'./dataset/{Config.DATASET}_Y.pkl', 'rb') as f:\n",
    "    Y = pickle.load(f)\n",
    "\n",
    "print(f\"\\nLoaded Dataset: {Config.DATASET}\")\n",
    "print(f\"  X shape: {X.shape}\")\n",
    "print(f\"  Y shape: {Y.shape if hasattr(Y, 'shape') else len(Y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Cell 4: File Parsing Helper Functions\n",
    "# ========================================================\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"Parse SHAP result filenames\"\"\"\n",
    "    dataset = Config.DATASET\n",
    "    \n",
    "    # SHAP values pattern\n",
    "    pattern_sv = re.compile(\n",
    "        rf'{dataset}_+([a-zA-Z0-9]+)_+(\\d+)_+(\\d+)_+(\\d+)(?:_+(?:chunk)?(\\d+))?_+sv_?\\.pkl'\n",
    "    )\n",
    "    \n",
    "    # Probability pattern\n",
    "    pattern_proba = re.compile(\n",
    "        rf'{dataset}_+([a-zA-Z0-9]+)_+(\\d+)_+(\\d+)(?:_+(?:chunk)?(\\d+))?_+proba_?\\.pkl'\n",
    "    )\n",
    "    \n",
    "    # Try SHAP values\n",
    "    match = pattern_sv.match(filename)\n",
    "    if match:\n",
    "        model, split, m_seed, e_seed, chunk_id = match.groups()\n",
    "        chunk_idx = int(chunk_id) if chunk_id is not None else -1\n",
    "        \n",
    "        return {\n",
    "            'type': 'sv', 'model': model,\n",
    "            'split': int(split), 'm_seed': int(m_seed), 'e_seed': int(e_seed),\n",
    "            'chunk_idx': chunk_idx, 'path': os.path.join(Config.RESULT_DIR, filename)\n",
    "        }\n",
    "    \n",
    "    # Try probabilities\n",
    "    match = pattern_proba.match(filename)\n",
    "    if match:\n",
    "        model, split, m_seed, chunk_id = match.groups()\n",
    "        chunk_idx = int(chunk_id) if chunk_id is not None else -1\n",
    "        \n",
    "        return {\n",
    "            'type': 'proba', 'model': model,\n",
    "            'split': int(split), 'm_seed': int(m_seed), 'e_seed': None,\n",
    "            'chunk_idx': chunk_idx, 'path': os.path.join(Config.RESULT_DIR, filename)\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def merge_chunks(file_list, return_meta=False):\n",
    "    \"\"\"Load and merge chunked files\"\"\"\n",
    "    if not file_list:\n",
    "        return (None, {}) if return_meta else None\n",
    "    \n",
    "    file_list.sort(key=lambda x: x['chunk_idx'])\n",
    "    merged_values = []\n",
    "    meta = {}\n",
    "    \n",
    "    first_obj = load_pickle(file_list[0]['path'])\n",
    "    if isinstance(first_obj, shap.Explanation) and return_meta:\n",
    "        meta['base_values'] = first_obj.base_values\n",
    "        meta['data'] = first_obj.data\n",
    "        meta['feature_names'] = first_obj.feature_names\n",
    "    \n",
    "    for item in file_list:\n",
    "        obj = load_pickle(item['path'])\n",
    "        val = obj.values if isinstance(obj, shap.Explanation) else obj\n",
    "        merged_values.append(val)\n",
    "    \n",
    "    full_values = np.concatenate(merged_values, axis=0)\n",
    "    \n",
    "    if return_meta and 'data' in meta and len(file_list) > 1:\n",
    "        merged_X = []\n",
    "        for item in file_list:\n",
    "            obj = load_pickle(item['path'])\n",
    "            if isinstance(obj, shap.Explanation):\n",
    "                merged_X.append(obj.data)\n",
    "        if merged_X:\n",
    "            meta['data'] = np.concatenate(merged_X, axis=0)\n",
    "    \n",
    "    return (full_values, meta) if return_meta else full_values\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Cell 5: Load SHAP Results into Arrays\n",
    "# ========================================================\n",
    "\n",
    "def load_model_results(model_name):\n",
    "    \"\"\"Load all SHAP results for a given model\"\"\"\n",
    "    print(f\"\\nüîÑ Loading model: {model_name}\")\n",
    "    \n",
    "    all_files = os.listdir(Config.RESULT_DIR)\n",
    "    sv_map = {}\n",
    "    proba_map = {}\n",
    "    \n",
    "    # Categorize files\n",
    "    for fname in all_files:\n",
    "        info = parse_filename(fname)\n",
    "        if info and info['model'] == model_name:\n",
    "            if info['type'] == 'sv':\n",
    "                key = (info['split'], info['m_seed'], info['e_seed'])\n",
    "                if key not in sv_map:\n",
    "                    sv_map[key] = []\n",
    "                sv_map[key].append(info)\n",
    "            elif info['type'] == 'proba':\n",
    "                key = (info['split'], info['m_seed'])\n",
    "                if key not in proba_map:\n",
    "                    proba_map[key] = []\n",
    "                proba_map[key].append(info)\n",
    "    \n",
    "    if not sv_map:\n",
    "        print(f\"  ‚ö†Ô∏è  No SHAP files found\")\n",
    "        return None\n",
    "    \n",
    "    # Get dimensions\n",
    "    feature_names = None\n",
    "    data_by_split = {}\n",
    "    base_values_map = {}\n",
    "    max_samples = 0\n",
    "    n_features = 0\n",
    "    \n",
    "    for (split, m_seed, e_seed), files in sv_map.items():\n",
    "        values, meta = merge_chunks(files, return_meta=True)\n",
    "        max_samples = max(max_samples, values.shape[0])\n",
    "        n_features = values.shape[1]\n",
    "        \n",
    "        if feature_names is None:\n",
    "            feature_names = meta.get('feature_names')\n",
    "        if split not in data_by_split:\n",
    "            data_by_split[split] = meta.get('data')\n",
    "        if (split, m_seed) not in base_values_map:\n",
    "            base_values_map[(split, m_seed)] = meta.get('base_values')\n",
    "    \n",
    "    print(f\"  üìè Dimensions: N={max_samples}, F={n_features}\")\n",
    "    \n",
    "    # Initialize arrays\n",
    "    shap_array = np.full(\n",
    "        (Config.N_SPLITS, Config.N_MODEL_SEEDS, Config.N_EXPLAINER_SEEDS,\n",
    "         max_samples, n_features),\n",
    "        np.nan, dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    n_classes = 1\n",
    "    if proba_map:\n",
    "        first_p = merge_chunks(next(iter(proba_map.values())))\n",
    "        if first_p.ndim > 1:\n",
    "            n_classes = first_p.shape[1]\n",
    "    \n",
    "    proba_array = np.full(\n",
    "        (Config.N_SPLITS, Config.N_MODEL_SEEDS, max_samples, n_classes),\n",
    "        np.nan, dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    # Fill arrays\n",
    "    for split in range(Config.N_SPLITS):\n",
    "        for m_seed in range(Config.N_MODEL_SEEDS):\n",
    "            if (split, m_seed) in proba_map:\n",
    "                p_data = merge_chunks(proba_map[(split, m_seed)])\n",
    "                if p_data.ndim == 1:\n",
    "                    p_data = p_data.reshape(-1, 1)\n",
    "                curr_n = p_data.shape[0]\n",
    "                proba_array[split, m_seed, :curr_n, :] = p_data\n",
    "    \n",
    "    for (split, m_seed, e_seed), files in sv_map.items():\n",
    "        values = merge_chunks(files, return_meta=False)\n",
    "        curr_n = values.shape[0]\n",
    "        shap_array[split, m_seed, e_seed, :curr_n, :] = values\n",
    "    \n",
    "    print(f\"  ‚úÖ {model_name} loaded\")\n",
    "    \n",
    "    return {\n",
    "        'shap_values': shap_array,\n",
    "        'proba': proba_array,\n",
    "        'data': data_by_split,\n",
    "        'base_values': base_values_map,\n",
    "        'feature_names': feature_names\n",
    "    }\n",
    "\n",
    "\n",
    "# Load all models\n",
    "print(\"=\"*60)\n",
    "print(\"Loading SHAP Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "raw_results = {}\n",
    "for model_name in Config.MODELS:\n",
    "    res = load_model_results(model_name)\n",
    "    if res:\n",
    "        raw_results[model_name] = res\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(raw_results)} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Cell 6: Generate Y_test Lists\n",
    "# ========================================================\n",
    "\n",
    "print(\"üîÑ Generating y_test lists...\")\n",
    "\n",
    "skf = StratifiedKFold(n_splits=Config.N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "Y_arr = Y.values if hasattr(Y, 'values') else Y\n",
    "X_arr = X.values if hasattr(X, 'values') else X\n",
    "\n",
    "y_test_list = []\n",
    "for _, test_index in skf.split(X_arr, Y_arr):\n",
    "    y_test_list.append(Y_arr[test_index])\n",
    "\n",
    "# Inject y_test\n",
    "for model_name in raw_results:\n",
    "    raw_results[model_name]['y_test'] = y_test_list\n",
    "\n",
    "print(\"‚úÖ Y_test lists generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Cell 7: Run Sensitivity Analysis\n",
    "# ========================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Running Sensitivity Analysis\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analysis_results = {}\n",
    "\n",
    "for model_name, raw_data in raw_results.items():\n",
    "    print(f\"\\nüîÑ Analyzing: {model_name}\")\n",
    "    \n",
    "    analyzer = SensitivityAnalyzer(\n",
    "        shap_5d=raw_data['shap_values'],\n",
    "        proba_4d=raw_data['proba'],\n",
    "        y_test_list=raw_data['y_test'],\n",
    "        feature_names=raw_data['feature_names']\n",
    "    )\n",
    "    \n",
    "    # Get masks for subgroups\n",
    "    cert_masks = analyzer.get_certainty_masks()\n",
    "    pred_masks = analyzer.get_prediction_masks()\n",
    "    \n",
    "    results = {\n",
    "        'overall_pooled': analyzer.compute_overall_pooled(),\n",
    "        'separated': analyzer.compute_separated_sensitivity(),\n",
    "        'seedwise_explainer': analyzer.compute_seedwise_explainer(),\n",
    "        'seedwise_model': analyzer.compute_seedwise_model(),\n",
    "        'certainty': analyzer.compute_certainty_subgroups(),\n",
    "        'prediction': analyzer.compute_prediction_subgroups(),\n",
    "        'certainty_separated': analyzer.compute_certainty_separated_sensitivity(),\n",
    "        'prediction_separated': analyzer.compute_prediction_separated_sensitivity(),\n",
    "        'feature_overall': {\n",
    "            'explainer': analyzer.compute_feature_sensitivity(mode='explainer'),\n",
    "            'model': analyzer.compute_feature_sensitivity(mode='model')\n",
    "        },\n",
    "        'feature_certainty': {},\n",
    "        'feature_prediction': {},\n",
    "        'certainty_feature_separated': analyzer.compute_certainty_feature_separated(),\n",
    "        'prediction_feature_separated': analyzer.compute_prediction_feature_separated(),\n",
    "        'feature_names': raw_data['feature_names'],\n",
    "        # Mean absolute SHAP values for y-axis labels\n",
    "        'mean_abs_shap': analyzer.compute_mean_abs_shap(),\n",
    "        'mean_abs_shap_certain': analyzer.compute_mean_abs_shap(subgroup_mask=cert_masks.get('Certain')),\n",
    "        'mean_abs_shap_uncertain': analyzer.compute_mean_abs_shap(subgroup_mask=cert_masks.get('Uncertain')),\n",
    "        'mean_abs_shap_tp': analyzer.compute_mean_abs_shap(subgroup_mask=pred_masks.get('TP')),\n",
    "        'mean_abs_shap_tn': analyzer.compute_mean_abs_shap(subgroup_mask=pred_masks.get('TN')),\n",
    "        'mean_abs_shap_fp': analyzer.compute_mean_abs_shap(subgroup_mask=pred_masks.get('FP')),\n",
    "        'mean_abs_shap_fn': analyzer.compute_mean_abs_shap(subgroup_mask=pred_masks.get('FN')),\n",
    "        # Probability data for plot 4\n",
    "        'proba_4d': raw_data['proba'],\n",
    "    }\n",
    "    \n",
    "    # Feature analyses for subgroups\n",
    "    for grp, masks in cert_masks.items():\n",
    "        results['feature_certainty'][grp] = analyzer.compute_feature_sensitivity(\n",
    "            mode='explainer', subgroup_mask=masks\n",
    "        )\n",
    "    \n",
    "    for grp, masks in pred_masks.items():\n",
    "        results['feature_prediction'][grp] = analyzer.compute_feature_sensitivity(\n",
    "            mode='explainer', subgroup_mask=masks\n",
    "        )\n",
    "    \n",
    "    analysis_results[model_name] = results\n",
    "    print(f\"  ‚úÖ {model_name} complete\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ All analyses complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Plots\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1-a: Overall Sensitivity (All 25 seeds pooled)\n",
    "# With baselines (shaded regions) for all metrics\n",
    "plot_1a_overall_pooled(analysis_results, metric='l2', show_baseline=True)\n",
    "plot_1a_overall_pooled(analysis_results, metric='jaccard', show_baseline=True)\n",
    "plot_1a_overall_pooled(analysis_results, metric='rbo', show_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1-b: Separated Sensitivity (Explainer vs Model)\n",
    "# With baselines (shaded regions) for all metrics\n",
    "plot_1b_separated(analysis_results, metric='l2', show_baseline=True)\n",
    "plot_1b_separated(analysis_results, metric='jaccard', show_baseline=True)\n",
    "plot_1b_separated(analysis_results, metric='rbo', show_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1-b: Separated Sensitivity (Explainer vs Model)\n",
    "# With baselines (shaded regions) for all metrics\n",
    "plot_1b_separated(analysis_results, metric='l2', show_baseline=True)\n",
    "plot_1b_separated(analysis_results, metric='jaccard', show_baseline=True)\n",
    "plot_1b_separated(analysis_results, metric='rbo', show_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Plot 1-c: Seed-wise Decomposition (all models, all metrics)\n",
    "# With baselines for all metrics (L2, Jaccard, RBO)\n",
    "# ========================================================\n",
    "for model_name in Config.MODELS:\n",
    "    for metric in ['l2', 'jaccard', 'rbo']:\n",
    "        plot_1c_seedwise(analysis_results, model_name, metric=metric, sensitivity_type='explainer', show_baseline=True)\n",
    "        plot_1c_seedwise(analysis_results, model_name, metric=metric, sensitivity_type='model', show_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Plot 1-d: Feature-level Overall Sensitivity\n",
    "# Shows which features are most unstable (largest mean absolute differences)\n",
    "# ========================================================\n",
    "for model_name in Config.MODELS:\n",
    "    plot_1d_feature_overall(analysis_results, model_name, mode='explainer', use_boxplot=True)\n",
    "    plot_1d_feature_overall(analysis_results, model_name, mode='model', use_boxplot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2-a: Certainty-based Sensitivity (Explainer Only)\n",
    "# With baselines (shaded regions) for all metrics\n",
    "plot_2a_certainty(analysis_results, metric='l2', show_baseline=True)\n",
    "plot_2a_certainty(analysis_results, metric='jaccard', show_baseline=True)\n",
    "plot_2a_certainty(analysis_results, metric='rbo', show_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2-a: Certainty-based Sensitivity (Explainer Only)\n",
    "# With baselines (shaded regions) for all metrics\n",
    "plot_2a_certainty(analysis_results, metric='l2', show_baseline=True)\n",
    "plot_2a_certainty(analysis_results, metric='jaccard', show_baseline=True)\n",
    "plot_2a_certainty(analysis_results, metric='rbo', show_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2-b: Certainty Feature Sensitivity (Explainer Only)\n",
    "# Distinct colors: Certain=blue, Uncertain=red\n",
    "for model_name in Config.MODELS:\n",
    "    plot_2b_certainty_features(analysis_results, model_name, subgroup='Certain')\n",
    "    plot_2b_certainty_features(analysis_results, model_name, subgroup='Uncertain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot 3-a: Prediction-based Sensitivity (Explainer Only)\n",
    "# With baselines (shaded regions) for all metrics\n",
    "plot_3a_prediction(analysis_results, metric='l2', show_baseline=True)\n",
    "plot_3a_prediction(analysis_results, metric='jaccard', show_baseline=True)\n",
    "plot_3a_prediction(analysis_results, metric='rbo', show_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3-a: Prediction-based Sensitivity (Explainer Only)\n",
    "# With baselines (shaded regions) for all metrics\n",
    "plot_3a_prediction(analysis_results, metric='l2', show_baseline=True)\n",
    "plot_3a_prediction(analysis_results, metric='jaccard', show_baseline=True)\n",
    "plot_3a_prediction(analysis_results, metric='rbo', show_baseline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3-b: Prediction Feature Sensitivity (Explainer Only)\n",
    "# Distinct colors: TP=green, TN=blue, FP=red, FN=orange\n",
    "for model_name in Config.MODELS:\n",
    "    for subgroup in ['TP', 'TN', 'FP', 'FN']:\n",
    "        plot_3b_prediction_features(analysis_results, model_name, subgroup=subgroup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Plot 3-b shows feature-wise sensitivity for prediction subgroups (explainer seeds only). Model seeds removed because they change prediction masks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "# Plot 4: Probability Distribution by Model Seeds\n",
    "# Shows how prediction probabilities vary across different model seeds\n",
    "# ========================================================\n",
    "for model_name in Config.MODELS:\n",
    "    plot_4_probability_distribution(analysis_results, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# # Model Îç∞Ïù¥ÌÑ∞ Î∂ÑÌè¨ ÌôïÏù∏\n",
    "# model_data = analysis_results['dt']['separated']['model']['jaccard']\n",
    "\n",
    "# fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# # ÌûàÏä§ÌÜ†Í∑∏Îû®\n",
    "# axes[0].hist(model_data, bins=30, edgecolor='black', alpha=0.7, color='green')\n",
    "# axes[0].set_xlabel('Jaccard Distance')\n",
    "# axes[0].set_ylabel('Count')\n",
    "# axes[0].set_title('Model Data - Histogram')\n",
    "# axes[0].axvline(0, color='red', linestyle='--', label='Zero')\n",
    "# axes[0].legend()\n",
    "\n",
    "# # ÌÜµÍ≥Ñ Ï†ïÎ≥¥\n",
    "# stats_text = f\"\"\"Model Data Statistics:\n",
    "\n",
    "# Min: {np.min(model_data):.4f}\n",
    "# Max: {np.max(model_data):.4f}\n",
    "# Mean: {np.mean(model_data):.4f}\n",
    "# Median: {np.median(model_data):.4f}\n",
    "# Std: {np.std(model_data):.4f}\n",
    "\n",
    "# Q1 (25%): {np.percentile(model_data, 25):.4f}\n",
    "# Q3 (75%): {np.percentile(model_data, 75):.4f}\n",
    "\n",
    "# Count = 0: {np.sum(model_data == 0)}\n",
    "# Count < 0.1: {np.sum(model_data < 0.1)}\n",
    "# Count < 0.2: {np.sum(model_data < 0.2)}\n",
    "# \"\"\"\n",
    "\n",
    "# axes[1].text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center', family='monospace')\n",
    "# axes[1].axis('off')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# print(f\"\\nData points: {len(model_data)}\")\n",
    "# print(f\"% below 0.1: {100 * np.sum(model_data < 0.1) / len(model_data):.1f}%\")\n",
    "# print(f\"% below 0.2: {100 * np.sum(model_data < 0.2) / len(model_data):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
